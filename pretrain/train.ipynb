{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "576e9afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/DATA-4/hx/conda_envs/UNI/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import timm\n",
    "import h5py\n",
    "import anndata as ad\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys \n",
    "import torch\n",
    "from torch import einsum\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "sys.path.append(\"/mnt/DATA-4/hx/Ruipath/MunchkinCat\")\n",
    "sys.path.append(\"/mnt/DATA-4/hx/Ruipath/scFoundation/model/\")\n",
    "# from model_finetune import *\n",
    "from data_loader import *\n",
    "from load import main_gene_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2821f0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_ids, slide_patch_ids = Get_hest_meta()\n",
    "#mydataset = HESTDataset(ids, root_dir=\"/mnt/DATA-4/hx/Ruipath/hest_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f44140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = HESTDataset(\n",
    "    data_root=\"/mnt/DATA-4/hx/Ruipath/hest_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fa2b6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# # Provide a patch image from a WSI\n",
    "# patch_image = Image.open(\"wsi_patch.png\")\n",
    "# image = transform(patch_image).unsqueeze(0)\n",
    "# with torch.inference_mode():\n",
    "#     patch_feature = model(image).cpu().numpy().astype(np.float32)\n",
    "#     print(patch_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e4003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuiPathViT(nn.Module):\n",
    "\n",
    "    def __init__(self, ckpt_path):\n",
    "        super().__init__()\n",
    "        self.ckpt_path = ckpt_path\n",
    "        self.head_lr = head_lr\n",
    "\n",
    "    def build(self):\n",
    "        self.model = timm.create_model(\n",
    "            \"vit_large_patch16_224\", \n",
    "            img_size=224, \n",
    "            patch_size=16, \n",
    "            init_values=1e-5, \n",
    "            num_classes=0, \n",
    "            dynamic_img_size=True\n",
    "        )\n",
    "        embed_dim = self.model.num_features\n",
    "        self.head = nn.Sequential(\n",
    "                nn.LayerNorm(embed_dim),\n",
    "                nn.Linear(embed_dim, embed_dim*2),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(embed_dim*2, embed_dim)\n",
    "            )\n",
    "        self.model.load_state_dict(torch.load(self.ckpt_path, map_location=device), strict=True)\n",
    "        for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        features = self.model(x, *args, **kwargs)\n",
    "        return self.head(features)\n",
    "    \n",
    "    def construct_optimizers(self, head_lr=1e-3):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.head.parameters(),\n",
    "            lr=head_lr,\n",
    "            weight_decay=1e-2,\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4fd3884",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = \"/mnt/DATA-4/hx/Ruipath/RuiPathViT/Ruipath_visionfoundation_v1.bin\"\n",
    "ruipathvit = RuiPathViT(local_dir)\n",
    "ruipathvit.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b869c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"/mnt/DATA-4/hx/Ruipath/scFoundation/model/\") # path to this folder\n",
    "from load import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d234a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class scFoundation(nn.Module):\n",
    "\n",
    "    def __init__(self, ckpt_path, out_dim, frozenmore=True):\n",
    "        super().__init__()\n",
    "        self.ckpt_path = ckpt_path\n",
    "        self.frozenmore = frozenmore\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "    def build(self):\n",
    "        model,model_config = load_model_frommmf(self.ckpt_path)\n",
    "        self.token_emb = model.token_emb\n",
    "        self.pos_emb = model.pos_emb\n",
    "        self.encoder = model.encoder\n",
    "        \n",
    "        if self.frozenmore:\n",
    "            for _,p in self.token_emb.named_parameters():\n",
    "                p.requires_grad = False\n",
    "            for _,p in self.pos_emb.named_parameters():\n",
    "                p.requires_grad = False\n",
    "            print('self.pos_emb and self.token_emb also frozen')\n",
    "        \n",
    "        for na, param in self.encoder.named_parameters():\n",
    "            param.requires_grad = False\n",
    "        for na, param in self.encoder.transformer_encoder[-2].named_parameters():\n",
    "            print('self.encoder.transformer_encoder ',na,' have grad')\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "        nn.Linear(model_config['encoder']['hidden_dim'], self.out_dim*2),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.out_dim*2, self.out_dim)\n",
    "        ) \n",
    "        self.norm = torch.nn.BatchNorm1d(model_config['encoder']['hidden_dim'], affine=False, eps=1e-6)\n",
    "        self.model_config = model_config\n",
    "        \n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        \n",
    "        value_labels = x > 0\n",
    "        x, x_padding = gatherData(x, value_labels, self.model_config['pad_token_id'])\n",
    "        data_gene_ids = torch.arange(19264, device=x.device).repeat(x.shape[0], 1)\n",
    "        position_gene_ids, _ = gatherData(data_gene_ids, value_labels,\n",
    "                                        self.model_config['pad_token_id'])\n",
    "        \n",
    "        x = self.token_emb(torch.unsqueeze(x, 2).float(), output_weight = 0)\n",
    "        position_emb = self.pos_emb(position_gene_ids)\n",
    "        x += position_emb\n",
    "\n",
    "        geneemb = self.encoder(x,x_padding)\n",
    "        geneembmerge, _ = torch.max(geneemb, dim=1)\n",
    "        \n",
    "        return self.fc1(geneembmerge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f092181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuiPathST(nn.Module):\n",
    "    def __init__(self, vision_model, sc_model):\n",
    "        super().__init__()\n",
    "        self.vision_model = vision_model\n",
    "        self.sc_model = sc_model\n",
    "        self.temperature = nn.Parameter(torch.Tensor([1.]))\n",
    "    \n",
    "    def embed_img(self, img):\n",
    "        return self.vision_model(img)\n",
    "    \n",
    "    def embed_gene(self, gene_expr):\n",
    "        return self.sc_model(gene_expr)\n",
    "\n",
    "    def forward(self, img, gene_expr):\n",
    "        batch_size = img.shape[0]\n",
    "        img_features = self.embed_img(img)\n",
    "        gene_features = self.embed_gene(gene_expr)\n",
    "        ce = F.cross_entropy\n",
    "        sim = einsum('i d, j d -> i j', img_features, gene_features)\n",
    "        sim = sim * self.temperature.exp()\n",
    "        contrastive_labels = torch.arange(batch_size, device=device)\n",
    "\n",
    "        contrastive_loss = (ce(sim, contrastive_labels) + ce(sim.t(), contrastive_labels)) * 0.5\n",
    "        return contrastive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "14c6a41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RuipathST = RuiPathST(ruipathvit, sc_model).cuda()\n",
    "RuipathST(img.unsqueeze(0).cuda(), x.unsqueeze(0).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a2eefcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [1/3157],                   Total Loss: 0.0351, Avg Loss: 3.5122\n",
      "Epoch [1/3], Step [4/3157],                   Total Loss: 0.4263, Avg Loss: 10.6580\n",
      "Epoch [1/3], Step [7/3157],                   Total Loss: 0.7644, Avg Loss: 10.9200\n",
      "Epoch [1/3], Step [10/3157],                   Total Loss: 1.1432, Avg Loss: 11.4322\n",
      "Epoch [1/3], Step [13/3157],                   Total Loss: 1.2732, Avg Loss: 9.7938\n",
      "Epoch [1/3], Step [16/3157],                   Total Loss: 1.3912, Avg Loss: 8.6950\n",
      "Epoch [1/3], Step [19/3157],                   Total Loss: 1.5028, Avg Loss: 7.9094\n",
      "Epoch [1/3], Step [22/3157],                   Total Loss: 1.6193, Avg Loss: 7.3606\n",
      "Epoch [1/3], Step [25/3157],                   Total Loss: 1.7420, Avg Loss: 6.9681\n",
      "Epoch [1/3], Step [28/3157],                   Total Loss: 1.8553, Avg Loss: 6.6261\n",
      "Epoch [1/3], Step [31/3157],                   Total Loss: 1.9865, Avg Loss: 6.4079\n",
      "Epoch [1/3], Step [34/3157],                   Total Loss: 2.0981, Avg Loss: 6.1708\n",
      "Epoch [1/3], Step [37/3157],                   Total Loss: 2.2103, Avg Loss: 5.9737\n",
      "Epoch [1/3], Step [40/3157],                   Total Loss: 2.3166, Avg Loss: 5.7916\n",
      "Epoch [1/3], Step [43/3157],                   Total Loss: 2.4218, Avg Loss: 5.6322\n",
      "Epoch [1/3], Step [46/3157],                   Total Loss: 2.5270, Avg Loss: 5.4934\n",
      "Epoch [1/3], Step [49/3157],                   Total Loss: 2.6323, Avg Loss: 5.3719\n",
      "Epoch [1/3], Step [52/3157],                   Total Loss: 2.7368, Avg Loss: 5.2630\n",
      "Epoch [1/3], Step [55/3157],                   Total Loss: 2.8410, Avg Loss: 5.1655\n",
      "Epoch [1/3], Step [58/3157],                   Total Loss: 2.9454, Avg Loss: 5.0782\n",
      "Epoch [1/3], Step [61/3157],                   Total Loss: 3.0504, Avg Loss: 5.0007\n",
      "Epoch [1/3], Step [64/3157],                   Total Loss: 3.1547, Avg Loss: 4.9293\n",
      "Epoch [1/3], Step [67/3157],                   Total Loss: 3.2589, Avg Loss: 4.8640\n",
      "Epoch [1/3], Step [70/3157],                   Total Loss: 3.3630, Avg Loss: 4.8043\n",
      "Epoch [1/3], Step [73/3157],                   Total Loss: 3.4670, Avg Loss: 4.7493\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m model(images, genes)  \u001b[38;5;66;03m# 假设模型这样设计\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/mnt/DATA-4/hx/conda_envs/UNI/lib/python3.10/site-packages/torch/_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    624\u001b[0m     )\n\u001b[0;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/DATA-4/hx/conda_envs/UNI/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/DATA-4/hx/conda_envs/UNI/lib/python3.10/site-packages/torch/autograd/graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)   \n",
    "model = RuiPathST(ruipathvit, sc_model).cuda()\n",
    "\n",
    "# 使用自定义 criterion（这里不需要 nn.CrossEntropyLoss）\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (images, genes) in enumerate(train_loader):  # 假设 dataloader 返回图像+基因\n",
    "        images = images.cuda()\n",
    "        genes = genes.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(images, genes)  # 假设模型这样设计\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        num_batches = i + 1\n",
    "        avg_loss = running_loss / num_batches\n",
    "        if i % 3 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{3}], Step [{i+1}/{len(train_loader)}], \\\n",
    "                  Total Loss: {running_loss:.4f}, Avg Loss: {avg_loss:.4f}')\n",
    "\n",
    "    # 验证部分（可选，InfoNCE 通常不直接验证 accuracy）\n",
    "    print(f\"Loss on epoch {epoch+1}: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"/mnt/DATA-4/hx/Ruipath/RuiPathST_ckp\")\n",
    "print(f\"Model saved to /mnt/DATA-4/hx/Ruipath/RuiPathST_ckp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UNI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
